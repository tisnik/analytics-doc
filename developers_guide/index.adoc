= Fabric8 Analytics Developer's Guide
:icons: font
Pavel Tišnovský <ptisnovs@redhat.com>
v1.0, 2018-10-18

== API Documentation

include::api_documentation.adoc[]

== Contributing

NOTE: First off, thanks for taking the time to contribute!

The following is a set of guidelines (not rules) for contributing to Fabric8-Analytics,
which is hosted in the https://github.com/fabric8-analytics/[Fabric8-Analytics Organization] on Github.
These are just guidelines, not rules, use your best judgment and feel free to
propose changes to this document in a pull request.

=== Submitting Issues

* You can create an issue on any repo under https://github.com/fabric8-analytics[Fabric8-analytics Github org], include as many details as possible with your report
* Include the behavior you expected and maybe other places you've seen that behavior

=== Submitting a Pull Request

NOTE: Every PR requires at least one review from at least one of the Core Reviewers member.

Core Reviewers are:

* mailto:msrb@redhat.com[Michal Srb]
* mailto:thrcka@redhat.com[Tomas Hrcka]
* mailto:ptisnovs@redhat.com[Pavel Tisnovsky]

Before you submit your pull request consider the following guidelines:

* Fork the repository and clone your fork
* Make your changes in a new git branch:

[source,bash]
----
git checkout -b bug/my-fix-branch master
----

* Create your patch, **ideally including appropriate test cases**
* Include documentation that either describe a change to a behavior or the changed capability to an end user
* Commit your changes using **a descriptive commit message**. If you are fixing an issue please include something like "this closes issue #xyz"
* Make sure your tests pass! We use Jenkins CI for our automated testing
* Push your branch to GitHub:

[source,bash]
----
git push origin bug/my-fix-branch
----

* When opening a pull request, select the `master` branch as a base.
* Mark your pull request with **[WIP]** (Work In Progress) to get feedback but prevent merging (e.g. [WIP] Update CONTRIBUTING.md)
* If we suggest changes then:
** Make the required updates
** Push changes to git (this will update your Pull Request):
** * You can add new commit
** * Or rebase your branch and force push to your Github repository:

----
git rebase -i master
git push -f origin bug/my-fix-branch
----

That's it! Thank you for your contribution!

=== Merge Rules

* Include unit or integration tests for the capability you have implemented
* Include documentation for the capability you have implemented
* If you are fixing an issue, include the issue number you are fixing
* Python code should follow https://www.python.org/dev/peps/pep-0008/[PEP8] conventions

=== Git Commit Messages

* Use the present tense ("Add feature" not "Added feature")
* Use the imperative mood ("Move cursor to..." not "Moves cursor to...")
* Reference issues and pull requests liberally

== Implementation details

=== API

* Use hyphenation over underscore or camelCase (i.e. `/my-awesome-endpoint`)
* Provide extensive examples for input and output
* Payload transferred over API should be in JSON format (exceptions are possible - for example while transferring files) and has to be documented with http://json-schema.org/[JSON Schema] and https://jsl.readthedocs.io/en/latest/tutorial.html[JSL], see existing schemas for https://github.com/fabric8-analytics/fabric8-analytics-worker/tree/master/f8a_worker/workers/schemas[workers] and https://github.com/fabric8-analytics/fabric8-analytics-server/tree/master/bayesian/schemas[server]


=== Language

* Use Python 3 where possible with potential exceptions:
** Ecosystem (Node.js, Ruby...) specific features which require parsing of the non-Python code
** Specific use case where Python does not provide needed functionality, library, framework... - needs strong justification and approval from tech leads

== Continuous Integration

== Testing Fabric8-Analytics Code
The following tests can be run to test your code in Fabric8-Analytics. Detailed information about each type of test is included in the corresponding folder in the repository.

* Integration tests: The https://github.com/fabric8-analytics/fabric8-analytics-common/blob/master/integration-tests/integration_tests.adoc[integration_tests] for fabric8-analytics services can be run against existing deployment, or locally using docker-compose.

* Performance tests: A basic set of https://github.com/fabric8-analytics/fabric8-analytics-common/blob/master/perf-tests/performance_tests.adoc[performance_tests] measures the component search and stack analysis at this moment.

* UI tests: The https://github.com/fabric8-analytics/fabric8-analytics-common/blob/master/ui-tests/ui_tests.adoc[ui_tests] access OpenShift.io and check whether it is possible to create a new space, configure the project in that space, and get the stack and license analysis for this project.

* Database integrity tests: Database integrity tests are described in the https://github.com/fabric8-analytics/fabric8-analytics-common/blob/master/db-integrity-tests/database_integrity_tests.adoc[database_integrity_tests].


== QA and testing

=== Actual status

==== Integration tests

Priority: *high*

Integration tests are part of the https://github.com/fabric8-analytics/fabric8-analytics-common[fabric8-analytics-common] repository

They are based on the http://pythonhosted.org/behave/[Behave] project

===== Documentation

* Documentation is part of the https://github.com/fabric8-analytics/fabric8-analytics-common[fabric8-analytics-common] repository, see
 https://github.com/fabric8-analytics/fabric8-analytics-common/blob/master/integration-tests/README.md[README.md]
* Contains outdated informations
* Needs to be updated
* Test steps description are now generated and presented on page https://fabric8-analytics.github.io/common.html[fabric8-analytics.github.io]

===== Issues/TODO


===== Blockers


===== Done

==== End-to-end tests

Priority: *high*

===== Documentation
POC for EE tests : https://github.com/naina-verma/poc-tests-analytics/blob/master/README.md
All EE tests should go to: https://github.com/fabric8io/fabric8-test

===== Issues/TODO

===== Blockers
Due to Build pipeline cannot proceed with automating UI stack reports.


==== Unit tests

Priority: *normal*

Unit tests are part of sources for all components

===== Documentation

N/A (see Issues below)

===== Issues/TODO


===== Blockers

==== Performance tests

Priority: *normal*

* Performance tests are run against Core server (almighty-core)
* Performance tests are run against Jobs server (flow-analysis and so)
* CRUD tests for workitems
* Contact information: Pavel Macik

===== Test results

* https://github.com/fabric8-analytics/fabric8-analytics.github.io/tree/master/perf-tests

===== Documentation

===== Issues/TODO

===== Blockers


==== Code quality tests


===== Documentation

===== Issues/TODO


===== Blockers

==== Regression tests

Priority: *low*

N/A at this moment

===== Documentation

N/A

===== Issues/TODO

* TBD

===== Blockers

* TBD


=== Test plans

==== Overall

Move this document from wiki to the new repo? *fabric8-qa* or something like this?


==== End-to-end tests

* [&times;] Make sure that all tests are on https://github.com/fabric8io/fabric8-test
* [&times;] Dockerize the tests
* [&times;] Further cooperate with Leonard Dimaggio on plans for EE tests
* [&times;] Specify test scenarios

==== Integration tests

* [&#x2713;] Provide documentation of test steps
* [-] Display link to test results
* [-] Cooperate with devels to update API documentation
* [&#x2713;] Update https://github.com/fabric8-analytics/fabric8-analytics-common/blob/master/integration-tests/README.md
* [&#x2713;] Create smoketests
* [-] Create tests for all https://raw.githubusercontent.com/fabric8-analytics/fabric8-analytics.github.io/master/server-api.txt[server API calls]
* [-] Create tests for all https://raw.githubusercontent.com/fabric8-analytics/fabric8-analytics.github.io/master/jobs-api.txt[jobs API calls]
* [&times;] Create API test coverage report

==== Unit tests

* [-] Cooperate with devels to integrate tests into MR workflow
* [?] Possibly use GIT hooks for this task
* [-] https://github.com/fabric8-analytics/fabric8-analytics-server/issues/28
* [-] https://github.com/fabric8-analytics/fabric8-analytics-server/issues/29

==== Performance tests

* [-] Cooperate with Pavel Macik who is responsible for perftests for all components
* [-] Define stories for perftests for analytics modules

==== Code quality tests

* [-] The process needs to be refined
* [-] https://github.com/fabric8-analytics/fabric8-analytics-server/issues/30
* [-] Cooperate with devels to integrate tests into MR workflow
* [?] Possibly use GIT hooks for this task

==== Regression tests

* [-] Define the process
* [-] Cooperate with devels to follow the process
* [-] Update/create Jenkins jobs to perform regression tests


=== Test plans

=== Useful links

* http://pythontesting.net/start-here/[Introductions to Python Testing Frameworks]
* http://pythonhosted.org/behave/[Behave framework]
* https://docs.python.org/3.3/library/unittest.html#[unittest — Unit testing framework]
* http://docs.python-requests.org/en/master/[Requests: HTTP for Humans]
* https://en.wikipedia.org/wiki/Regression_testing[Regression testing]
* https://github.com/pmacik/ldimaggi-perfcake[Red Hat Developer Performance Tests]


== Coding standards

Use the following scripts to check if the code follows  https://www.python.org/dev/peps/pep-0008/[PEP8] and  https://www.python.org/dev/peps/pep-0257/[PEP257] coding standards. These scripts can be run without any arguments:

* `./run-linter.sh` : Use this script to check the indentation, line lengths, variable names, and white space around the operators.

* `./check-docstyle.sh`:  Use this script to check all documentation strings, their presence, and format.

Ensure that you fix any warnings and errors reported by these scripts.

== Code complexity measurement
Use the following scripts to measure code complexity. These scripts can be run w/o any arguments:

* `./measure-cyclomatic-complexity.sh`: Use this script to measure the  cyclomatic complexity of all the Python sources found in the repository. See this table for further explanation on interpreting the results.
* `./measure-maintainability-index.sh`:  Use this script to measure the maintainability index of all the Python sources found in the repository. See the explanation of this measurement for more details.

== Dead code detection

The script `detect-dead-code.sh` can be used to detect dead code in the repository. This script can be run w/o any arguments:

----
./detect-dead-code.sh
----

Please note that due to Python's dynamic nature, static code analyzers are likely to miss some dead code. Also, code that is only called implicitly may be reported as unused.

Because of this potential problems, only code detected with more than 90% of confidence is reported.

== Common issues detection

The script `detect-common-errors.sh` can be used to detect common errors in the repository. This script can be run w/o any arguments:

----
./detect-common-errors.sh
----

Please note that only semantical problems are reported.

